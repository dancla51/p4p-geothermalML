First made flat model using solver
Then made linear model using solver - both too slow to get good results

Then made flat model using .diff() and setting derivatives to 0s, forming matrix, inverting and solving.

26th June:

Then, When combining linear model with solving via differentiation, we get interesting results. 
The results are inaccurate but very feasible given the information provided to the system.
There is too many DOFs on the system?

Possibly introducing regularisation term would correct this, but could also overcorrect this. Same issue likely to occur for higher order polynomials.
Or is possible that hyperparameter tuning on lambda could fix this issue. Interestingly, when lambda=0, the matrix is singular. Why?
Could also constrain values based on pre-existing knowledge? 

I did some maths and found an expression for an "optimal" / "balanced" value for lambda = Tn^2/d ~= 100 for this data.
This value gives really good results when lots of data is provided (2 or 3 per well)


Ideas:
Introduce regularisation to 'flatten' results when not enough data provided (Ridge Regression)
Could we treat d (measured/not) as a non-binary variable (as in dij = 0.5 if the well was running for half of the month)?
Expand from linear to arbitrary polynomial with feature selection (LASSO Regression)